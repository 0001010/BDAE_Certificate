{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a2f86aba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>GraduateOrNot</th>\n",
       "      <th>AnnualIncome</th>\n",
       "      <th>FamilyMembers</th>\n",
       "      <th>ChronicDiseases</th>\n",
       "      <th>FrequentFlyer</th>\n",
       "      <th>EverTravelledAbroad</th>\n",
       "      <th>TravelInsurance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1250000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1250000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>Yes</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Yes</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1485</th>\n",
       "      <td>28</td>\n",
       "      <td>Yes</td>\n",
       "      <td>800000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>26</td>\n",
       "      <td>Yes</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>25</td>\n",
       "      <td>No</td>\n",
       "      <td>1150000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>31</td>\n",
       "      <td>Yes</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1490 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age GraduateOrNot  AnnualIncome  FamilyMembers  ChronicDiseases  \\\n",
       "0      28           Yes     1250000.0              6                1   \n",
       "1      31           Yes     1250000.0              7                1   \n",
       "2      29           Yes     1200000.0              7                0   \n",
       "3      33           Yes      650000.0              6                1   \n",
       "4      28           Yes      800000.0              6                0   \n",
       "...   ...           ...           ...            ...              ...   \n",
       "1485   28           Yes      800000.0              4                0   \n",
       "1486   34           Yes     1000000.0              9                0   \n",
       "1487   26           Yes      450000.0              5                1   \n",
       "1488   25            No     1150000.0              3                1   \n",
       "1489   31           Yes      400000.0              2                0   \n",
       "\n",
       "     FrequentFlyer EverTravelledAbroad  TravelInsurance  \n",
       "0               No                  No                0  \n",
       "1               No                  No                0  \n",
       "2               No                  No                1  \n",
       "3               No                  No                1  \n",
       "4               No                 Yes                1  \n",
       "...            ...                 ...              ...  \n",
       "1485            No                  No                0  \n",
       "1486            No                  No                0  \n",
       "1487            No                  No                1  \n",
       "1488            No                 Yes                0  \n",
       "1489            No                  No                0  \n",
       "\n",
       "[1490 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc 0.8119911176905996\n",
      "roc_gb 0.8006661732050333\n",
      "roc_xgb 0.8052306933135949\n",
      "0.7640062740638778\n",
      "0.7943628184411595\n",
      "0.8068169349851377\n",
      "Help on class XGBClassifier in module xgboost.sklearn:\n",
      "\n",
      "class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      " |  XGBClassifier(*, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: bool = False, **kwargs: Any) -> None\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for XGBoost classification.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |      n_estimators : int\n",
      " |          Number of boosting rounds.\n",
      " |  \n",
      " |      max_depth :  Optional[int]\n",
      " |          Maximum tree depth for base learners.\n",
      " |      max_leaves :\n",
      " |          Maximum number of leaves; 0 indicates no limit.\n",
      " |      max_bin :\n",
      " |          If using histogram-based algorithm, maximum number of bins per feature\n",
      " |      grow_policy :\n",
      " |          Tree growing policy. 0: favor splitting at nodes closest to the node, i.e. grow\n",
      " |          depth-wise. 1: favor splitting at nodes with highest loss change.\n",
      " |      learning_rate : Optional[float]\n",
      " |          Boosting learning rate (xgb's \"eta\")\n",
      " |      verbosity : Optional[int]\n",
      " |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      " |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |      booster: Optional[str]\n",
      " |          Specify which booster to use: gbtree, gblinear or dart.\n",
      " |      tree_method: Optional[str]\n",
      " |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
      " |          default, XGBoost will choose the most conservative option available.  It's\n",
      " |          recommended to study this option from the parameters document :doc:`tree method\n",
      " |          </treemethod>`\n",
      " |      n_jobs : Optional[int]\n",
      " |          Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      " |          algorithms like grid search, you may choose which algorithm to parallelize and\n",
      " |          balance the threads.  Creating thread contention will significantly slow down both\n",
      " |          algorithms.\n",
      " |      gamma : Optional[float]\n",
      " |          (min_split_loss) Minimum loss reduction required to make a further partition on a\n",
      " |          leaf node of the tree.\n",
      " |      min_child_weight : Optional[float]\n",
      " |          Minimum sum of instance weight(hessian) needed in a child.\n",
      " |      max_delta_step : Optional[float]\n",
      " |          Maximum delta step we allow each tree's weight estimation to be.\n",
      " |      subsample : Optional[float]\n",
      " |          Subsample ratio of the training instance.\n",
      " |      sampling_method :\n",
      " |          Sampling method. Used only by `gpu_hist` tree method.\n",
      " |            - `uniform`: select random training instances uniformly.\n",
      " |            - `gradient_based` select random training instances with higher probability when\n",
      " |              the gradient and hessian are larger. (cf. CatBoost)\n",
      " |      colsample_bytree : Optional[float]\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      colsample_bylevel : Optional[float]\n",
      " |          Subsample ratio of columns for each level.\n",
      " |      colsample_bynode : Optional[float]\n",
      " |          Subsample ratio of columns for each split.\n",
      " |      reg_alpha : Optional[float]\n",
      " |          L1 regularization term on weights (xgb's alpha).\n",
      " |      reg_lambda : Optional[float]\n",
      " |          L2 regularization term on weights (xgb's lambda).\n",
      " |      scale_pos_weight : Optional[float]\n",
      " |          Balancing of positive and negative weights.\n",
      " |      base_score : Optional[float]\n",
      " |          The initial prediction score of all instances, global bias.\n",
      " |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      " |          Random number seed.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      " |             it uses Hogwild algorithm.\n",
      " |  \n",
      " |      missing : float, default np.nan\n",
      " |          Value in the data which needs to be present as a missing value.\n",
      " |      num_parallel_tree: Optional[int]\n",
      " |          Used for boosting random forest.\n",
      " |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      " |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
      " |          for more information.\n",
      " |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      " |          Constraints for interaction representing permitted interactions.  The\n",
      " |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
      " |          3, 4]]``, where each inner list is a group of indices of features that are\n",
      " |          allowed to interact with each other.  See :doc:`tutorial\n",
      " |          </tutorials/feature_interaction_constraint>` for more information\n",
      " |      importance_type: Optional[str]\n",
      " |          The feature importance type for the feature_importances\\_ property:\n",
      " |  \n",
      " |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      " |            \"total_cover\".\n",
      " |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      " |            without bias.\n",
      " |  \n",
      " |      gpu_id : Optional[int]\n",
      " |          Device ordinal.\n",
      " |      validate_parameters : Optional[bool]\n",
      " |          Give warnings for unknown parameter.\n",
      " |      predictor : Optional[str]\n",
      " |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      " |          gpu_predictor].\n",
      " |      enable_categorical : bool\n",
      " |  \n",
      " |          .. versionadded:: 1.5.0\n",
      " |  \n",
      " |          .. note:: This parameter is experimental\n",
      " |  \n",
      " |          Experimental support for categorical data.  When enabled, cudf/pandas.DataFrame\n",
      " |          should be used to specify categorical data type.  Also, JSON/UBJSON\n",
      " |          serialization format is required.\n",
      " |  \n",
      " |      max_cat_to_onehot : Optional[int]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          .. note:: This parameter is experimental\n",
      " |  \n",
      " |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
      " |          for categorical data.  When number of categories is lesser than the threshold\n",
      " |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
      " |          into children nodes.  Only relevant for regression and binary classification.\n",
      " |          See :doc:`Categorical Data </tutorials/categorical>` for details.\n",
      " |  \n",
      " |      eval_metric : Optional[Union[str, List[str], Callable]]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          Metric used for monitoring the training result and early stopping.  It can be a\n",
      " |          string or list of strings as names of predefined metric in XGBoost (See\n",
      " |          doc/parameter.rst), one of the metrics in :py:mod:`sklearn.metrics`, or any other\n",
      " |          user defined metric that looks like `sklearn.metrics`.\n",
      " |  \n",
      " |          If custom objective is also provided, then custom metric should implement the\n",
      " |          corresponding reverse link function.\n",
      " |  \n",
      " |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
      " |          object is provided, it's assumed to be a cost function and by default XGBoost will\n",
      " |          minimize the result during early stopping.\n",
      " |  \n",
      " |          For advanced usage on Early stopping like directly choosing to maximize instead of\n",
      " |          minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
      " |  \n",
      " |          See :doc:`Custom Objective and Evaluation Metric </tutorials/custom_metric_obj>`\n",
      " |          for more.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |               This parameter replaces `eval_metric` in :py:meth:`fit` method.  The old one\n",
      " |               receives un-transformed prediction regardless of whether custom objective is\n",
      " |               being used.\n",
      " |  \n",
      " |          .. code-block:: python\n",
      " |  \n",
      " |              from sklearn.datasets import load_diabetes\n",
      " |              from sklearn.metrics import mean_absolute_error\n",
      " |              X, y = load_diabetes(return_X_y=True)\n",
      " |              reg = xgb.XGBRegressor(\n",
      " |                  tree_method=\"hist\",\n",
      " |                  eval_metric=mean_absolute_error,\n",
      " |              )\n",
      " |              reg.fit(X, y, eval_set=[(X, y)])\n",
      " |  \n",
      " |      early_stopping_rounds : Optional[int]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          Activates early stopping. Validation metric needs to improve at least once in\n",
      " |          every **early_stopping_rounds** round(s) to continue training.  Requires at least\n",
      " |          one item in **eval_set** in :py:meth:`fit`.\n",
      " |  \n",
      " |          The method returns the model from the last iteration (not the best one).  If\n",
      " |          there's more than one item in **eval_set**, the last entry will be used for early\n",
      " |          stopping.  If there's more than one metric in **eval_metric**, the last metric\n",
      " |          will be used for early stopping.\n",
      " |  \n",
      " |          If early stopping occurs, the model will have three additional fields:\n",
      " |          :py:attr:`best_score`, :py:attr:`best_iteration` and\n",
      " |          :py:attr:`best_ntree_limit`.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |              This parameter replaces `early_stopping_rounds` in :py:meth:`fit` method.\n",
      " |  \n",
      " |      callbacks : Optional[List[TrainingCallback]]\n",
      " |          List of callback functions that are applied at end of each iteration.\n",
      " |          It is possible to use predefined callbacks by using\n",
      " |          :ref:`Callback API <callback_api>`.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |             States in callback are not preserved during training, which means callback\n",
      " |             objects can not be reused for multiple training sessions without\n",
      " |             reinitialization or deepcopy.\n",
      " |  \n",
      " |          .. code-block:: python\n",
      " |  \n",
      " |              for params in parameters_grid:\n",
      " |                  # be sure to (re)initialize the callbacks before each run\n",
      " |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
      " |                  xgboost.train(params, Xy, callbacks=callbacks)\n",
      " |  \n",
      " |      kwargs : dict, optional\n",
      " |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
      " |          can be found :doc:`here </parameter>`.\n",
      " |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      " |          dict simultaneously will result in a TypeError.\n",
      " |  \n",
      " |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      " |  \n",
      " |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      " |              that parameters passed via this argument will interact properly\n",
      " |              with scikit-learn.\n",
      " |  \n",
      " |          .. note::  Custom objective function\n",
      " |  \n",
      " |              A custom objective function can be provided for the ``objective``\n",
      " |              parameter. In this case, it should have the signature\n",
      " |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |              y_true: array_like of shape [n_samples]\n",
      " |                  The target values\n",
      " |              y_pred: array_like of shape [n_samples]\n",
      " |                  The predicted values\n",
      " |  \n",
      " |              grad: array_like of shape [n_samples]\n",
      " |                  The value of the gradient for each sample point.\n",
      " |              hess: array_like of shape [n_samples]\n",
      " |                  The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBClassifier\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', use_label_encoder: bool = False, **kwargs: Any) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, eval_metric: Union[str, Sequence[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Optional[int] = None, verbose: Optional[bool] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None, callbacks: Optional[Sequence[xgboost.callback.TrainingCallback]] = None) -> 'XGBClassifier'\n",
      " |      Fit gradient boosting classifier.\n",
      " |      \n",
      " |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      " |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      " |      pass ``xgb_model`` argument.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X :\n",
      " |          Feature matrix\n",
      " |      y :\n",
      " |          Labels\n",
      " |      sample_weight :\n",
      " |          instance weights\n",
      " |      base_margin :\n",
      " |          global bias for each instance.\n",
      " |      eval_set :\n",
      " |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      " |          metrics will be computed.\n",
      " |          Validation metrics will help us track the performance of the model.\n",
      " |      \n",
      " |      eval_metric : str, list of str, or callable, optional\n",
      " |          .. deprecated:: 1.6.0\n",
      " |              Use `eval_metric` in :py:meth:`__init__` or :py:meth:`set_params` instead.\n",
      " |      \n",
      " |      early_stopping_rounds : int\n",
      " |          .. deprecated:: 1.6.0\n",
      " |              Use `early_stopping_rounds` in :py:meth:`__init__` or\n",
      " |              :py:meth:`set_params` instead.\n",
      " |      verbose :\n",
      " |          If `verbose` and an evaluation set is used, writes the evaluation metric\n",
      " |          measured on the validation set to stderr.\n",
      " |      xgb_model :\n",
      " |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |      sample_weight_eval_set :\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      " |          object storing instance weights for the i-th validation set.\n",
      " |      base_margin_eval_set :\n",
      " |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      " |          object storing base margin for the i-th validation set.\n",
      " |      feature_weights :\n",
      " |          Weight for each feature, defines the probability of each feature being\n",
      " |          selected when colsample is being used.  All values must be greater than 0,\n",
      " |          otherwise a `ValueError` is thrown.\n",
      " |      \n",
      " |      callbacks :\n",
      " |          .. deprecated:: 1.6.0\n",
      " |              Use `callbacks` in :py:meth:`__init__` or :py:meth:`set_params` instead.\n",
      " |  \n",
      " |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      " |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n",
      " |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n",
      " |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n",
      " |      automatically, otherwise it will run on CPU.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X :\n",
      " |          Data to predict with.\n",
      " |      output_margin :\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      ntree_limit :\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      validate_features :\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin :\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
      " |          20)``, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |          .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction\n",
      " |  \n",
      " |  predict_proba(self, X: Any, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      " |      Predict the probability of each `X` example being of a given class.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like\n",
      " |          Feature matrix.\n",
      " |      ntree_limit : int\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      validate_features : bool\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin : array_like\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
      " |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
      " |          used in this prediction.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n",
      " |          probability of each data example being of a given class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __sklearn_is_fitted__(self) -> bool\n",
      " |  \n",
      " |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      " |      Return the predicted leaf every tree for each sample. If the model is trained with\n",
      " |      early stopping, then `best_iteration` is used automatically.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      iteration_range :\n",
      " |          See :py:meth:`predict`.\n",
      " |      \n",
      " |      ntree_limit :\n",
      " |          Deprecated, use ``iteration_range`` instead.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n",
      " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n",
      " |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n",
      " |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n",
      " |      function.\n",
      " |      \n",
      " |      The returned evaluation result is a dictionary:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result\n",
      " |  \n",
      " |  get_booster(self) -> xgboost.core.Booster\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_num_boosting_rounds(self) -> int\n",
      " |      Gets the number of xgboost boosting rounds.\n",
      " |  \n",
      " |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self) -> Dict[str, Any]\n",
      " |      Get xgboost specific parameters.\n",
      " |  \n",
      " |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      " |      Load the model from a file or bytearray. Path to file can be local\n",
      " |      or as an URI.\n",
      " |      \n",
      " |      The model is loaded from XGBoost format which is universal among the various\n",
      " |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      " |      feature_names) will not be loaded when using binary format.  To save those\n",
      " |      attributes, use JSON/UBJ instead.  See :doc:`Model IO </tutorials/saving_model>`\n",
      " |      for more info.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        model.load_model(\"model.json\")\n",
      " |        # or\n",
      " |        model.load_model(\"model.ubj\")\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname :\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal format which is universal among the\n",
      " |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      " |      (such as feature_names) will not be saved when using binary format.  To save\n",
      " |      those attributes, use JSON/UBJ instead. See :doc:`Model IO\n",
      " |      </tutorials/saving_model>` for more info.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        model.save_model(\"model.json\")\n",
      " |        # or\n",
      " |        model.save_model(\"model.ubj\")\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or os.PathLike\n",
      " |          Output file name\n",
      " |  \n",
      " |  set_params(self, **params: Any) -> 'XGBModel'\n",
      " |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      " |      allow unknown kwargs. This allows using the full range of xgboost\n",
      " |      parameters that are not defined as member variables in sklearn grid\n",
      " |      search.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from XGBModel:\n",
      " |  \n",
      " |  best_iteration\n",
      " |      The best iteration obtained by early stopping.  This attribute is 0-based,\n",
      " |      for instance if the best iteration is the first round, then best_iteration is 0.\n",
      " |  \n",
      " |  best_ntree_limit\n",
      " |  \n",
      " |  best_score\n",
      " |      The best score obtained by early stopping.\n",
      " |  \n",
      " |  coef_\n",
      " |      Coefficients property\n",
      " |      \n",
      " |      .. note:: Coefficients are defined only for linear learners\n",
      " |      \n",
      " |          Coefficients are only defined when the linear model is chosen as\n",
      " |          base learner (`booster=gblinear`). It is not defined for other base\n",
      " |          learner types, such as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Feature importances property, return depends on `importance_type` parameter.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
      " |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
      " |  \n",
      " |  feature_names_in_\n",
      " |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has feature\n",
      " |      names that are all strings.\n",
      " |  \n",
      " |  intercept_\n",
      " |      Intercept (bias) property\n",
      " |      \n",
      " |      .. note:: Intercept is defined only for linear learners\n",
      " |      \n",
      " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types,\n",
      " |          such as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      " |  \n",
      " |  n_features_in_\n",
      " |      Number of features seen during :py:meth:`fit`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import KBinsDiscretizer, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "train = pd.read_csv('./big-data-analytics-certification/t2-1-train.csv')\n",
    "test = pd.read_csv('./big-data-analytics-certification/t2-1-test.csv')\n",
    "sub = pd.read_csv('./big-data-analytics-certification/t2-1-sample_submission.csv')\n",
    "\n",
    "del train['id']\n",
    "ids = test['id']\n",
    "del test['id']\n",
    "del train['Employment Type']\n",
    "del test['Employment Type']\n",
    "# print(train.shape, test.shape)\n",
    "# print(train.info())\n",
    "# print()\n",
    "# print(test.info())\n",
    "display(train)\n",
    "# print(train['TravelInsurance'].value_counts(normalize = True))\n",
    "# class-imbalanced\n",
    "\n",
    "def kbs(feature, n_bins):\n",
    "    kb = KBinsDiscretizer(n_bins = n_bins, encode = 'ordinal')\n",
    "    kb.fit(train[['Age']])\n",
    "    train[feature] = kb.transform(train[[feature]])\n",
    "    test[feature] = kb.transform(test[[feature]])\n",
    "\n",
    "def label_encoder(feature):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train[feature])\n",
    "    train[feature] = le.transform(train[feature])\n",
    "    test[feature] = le.transform(test[feature])\n",
    "\n",
    "train = train.fillna(train['AnnualIncome'].mean())\n",
    "test = test.fillna(train['AnnualIncome'].mean())\n",
    "train['Age'] = train['Age']//10\n",
    "test['Age'] = test['Age']//10\n",
    "label_encoder('GraduateOrNot')\n",
    "label_encoder('FrequentFlyer')\n",
    "label_encoder('EverTravelledAbroad')\n",
    "sts = StandardScaler()\n",
    "train['AnnualIncome'] = np.log(train['AnnualIncome'])\n",
    "test['AnnualIncome'] = np.log(test['AnnualIncome'])\n",
    "\n",
    "\n",
    "train = pd.get_dummies(train, columns = ['Age'])\n",
    "test = pd.get_dummies(test, columns = ['Age'])\n",
    "\n",
    "y = train['TravelInsurance']\n",
    "del train['TravelInsurance']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, y, test_size = 0.2, random_state = 123456, stratify = y)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 75, random_state = 12346789)\n",
    "gb = GradientBoostingClassifier(learning_rate = 0.05, n_estimators = 50, max_depth = 5, random_state = 123456789)\n",
    "xgb = XGBClassifier(n_estimators = 30, learning_rate = 0.07, random_state = 123456789)\n",
    "rf.fit(x_train, y_train)\n",
    "gb.fit(x_train, y_train)\n",
    "xgb.fit(x_train, y_train)\n",
    "pred = rf.predict_proba(x_test)[:,1]\n",
    "pred_gb = gb.predict_proba(x_test)[:,1]\n",
    "pred_xgb = xgb.predict_proba(x_test)[:,1]\n",
    "\n",
    "#print('acc', accuracy_score(y_test, pred))\n",
    "print('roc', roc_auc_score(y_test, pred))\n",
    "\n",
    "#print('acc_gb', accuracy_score(y_test, pred_gb))\n",
    "print('roc_gb', roc_auc_score(y_test, pred_gb))\n",
    "print('roc_xgb', roc_auc_score(y_test, pred_xgb))\n",
    "\n",
    "print(cross_val_score(rf, x_train, y_train, cv = 5, scoring = 'roc_auc').mean())\n",
    "\n",
    "print(cross_val_score(gb, x_train, y_train, cv = 5, scoring = 'roc_auc').mean())\n",
    "\n",
    "print(cross_val_score(xgb, x_train, y_train, cv = 5, scoring = 'roc_auc').mean())\n",
    "\n",
    "# voting = VotingClassifier([('rf',rf),\n",
    "#                           ('gb', gb),\n",
    "#                           ('xgb', xgb)], voting = 'hard')\n",
    "\n",
    "# voting.fit(x_train, y_train)\n",
    "# pred_voting = voting.predict_proba(x_test)[:,1]\n",
    "# # print('acc_voting', accuracy_score(y_test, pred_voting))\n",
    "# print('roc_voting', roc_auc_score(y_test, pred_voting))\n",
    "\n",
    "# print(cross_val_score(voting, x_train, y_train, cv = 5, scoring = 'roc_auc').mean())\n",
    "\n",
    "\n",
    "pred = xgb.predict_proba(test)[:,1]\n",
    "sub['TravelInsurance'] = pred\n",
    "sub.to_csv('./sub.csv', index = False)\n",
    "\n",
    "help(XGBClassifier)\n",
    "# import sklearn as sp\n",
    "# dir(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765462e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
